{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BitWeavre/DRL_Classification_Projects/blob/main/DQN_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download data to Colab"
      ],
      "metadata": {
        "id": "y2Xd1kESoqj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment URLs to download all files\n",
        "urls = [\n",
        "    'https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz',\n",
        "    'https://nihcc.box.com/shared/static/i28rlmbvmfjbl8p2n3ril0pptcmcu9d1.gz',\n",
        "    'https://nihcc.box.com/shared/static/f1t00wrtdk94satdfb9olcolqx20z2jp.gz',\n",
        "    'https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz',\n",
        "    'https://nihcc.box.com/shared/static/v5e3goj22zr6h8tzualxfsqlqaygfbsn.gz',\n",
        "    'https://nihcc.box.com/shared/static/asi7ikud9jwnkrnkj99jnpfkjdes7l6l.gz',\n",
        "    # 'https://nihcc.box.com/shared/static/jn1b4mw4n6lnh74ovmcjb8y48h8xj07n.gz',\n",
        "    # 'https://nihcc.box.com/shared/static/tvpxmn7qyrgl0w8wfh9kqfjskv6nmm1j.gz',\n",
        "    # 'https://nihcc.box.com/shared/static/upyy3ml7qdumlgk2rfcvlb9k6gvqq2pj.gz',\n",
        "    # 'https://nihcc.box.com/shared/static/l6nilvfa9cg3s28tqv1qc1olm3gnz54p.gz',\n",
        "    # 'https://nihcc.box.com/shared/static/hhq8fkdgvcari67vfhs7ppg2w6ni4jze.gz',\n",
        "    # 'https://nihcc.box.com/shared/static/ioqwiy20ihqwyr8pf4c24eazhh281pbu.gz'\n",
        "]"
      ],
      "metadata": {
        "id": "hOMztlxwotxZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the directory where all the data will be placed"
      ],
      "metadata": {
        "id": "uRbil-cyvhjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.mkdir('all-data')"
      ],
      "metadata": {
        "id": "qLCDco27pTpP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the data, extract the file, place the images in ./all-data/ and remove the zip files to save space"
      ],
      "metadata": {
        "id": "H-y9k-TavkpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, url in enumerate(urls):\n",
        "    filename = url.rsplit('/', maxsplit=1)[1]\n",
        "\n",
        "    print(f'Downloading data file {idx + 1}/{len(urls)} ....')\n",
        "    os.system(f'wget {url}')\n",
        "\n",
        "    print(f'Extracting data file {idx + 1}/{len(urls)} ....')\n",
        "    os.system(f'tar -xvzf {filename}')\n",
        "\n",
        "    print(f'Moving images of data file {idx + 1}/{len(urls)} to ./all-data/ ....')\n",
        "    os.system(f'mv images/* all-data/')\n",
        "\n",
        "    print(f'Removing zipped data file {idx + 1}/{len(urls)} ....')\n",
        "    os.unlink(filename)\n",
        "    os.system(f'rm -rf images')\n",
        "\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIrM44vRoyEh",
        "outputId": "a40b8287-8646-4060-ad2a-52b3fe75de0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data file 1/6 ....\n",
            "Extracting data file 1/6 ....\n",
            "Moving images of data file 1/6 to ./all-data/ ....\n",
            "Removing zipped data file 1/6 ....\n",
            "\n",
            "\n",
            "Downloading data file 2/6 ....\n",
            "Extracting data file 2/6 ....\n",
            "Moving images of data file 2/6 to ./all-data/ ....\n",
            "Removing zipped data file 2/6 ....\n",
            "\n",
            "\n",
            "Downloading data file 3/6 ....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_downloads = len(os.listdir('all-data'))\n",
        "print(f'Number of images download: {num_downloads}')"
      ],
      "metadata": {
        "id": "zzzV-Keb1bE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download metadata file"
      ],
      "metadata": {
        "id": "gxl0zCFz8j5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/ingus-t/SPAI/master/resources/Data_Entry_2017.csv"
      ],
      "metadata": {
        "id": "2hfnRA138l0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN4cOiFucmEv"
      },
      "source": [
        "Download split list files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMh1UviJnPCP"
      },
      "outputs": [],
      "source": [
        "train_val_list_url = 'https://nihcc.app.box.com/v/ChestXray-NIHCC/file/256056636701'\n",
        "test_list_url = 'https://nihcc.app.box.com/v/ChestXray-NIHCC/file/256055473534'\n",
        "\n",
        "download_button_xpath = '/html/body/div[1]/div[5]/span/div/span/div/header/div[2]/button[2]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drwWdgcPcMiE"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfJK0OPXcMfF"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUPUj2agCowa"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver', options=chrome_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyL9IF8hB3Vb"
      },
      "outputs": [],
      "source": [
        "def download_file_from_box(url):\n",
        "    retries = 5\n",
        "    while retries > 0:\n",
        "        try:\n",
        "            driver.get(url)\n",
        "            download_button = driver.find_element('xpath', download_button_xpath)\n",
        "            download_button.click()\n",
        "            break\n",
        "        except:\n",
        "            retries -= 1\n",
        "    if retries == 0:\n",
        "        raise RuntimeError(f'Could not fetch data from url: {url}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYrMHae6dXqc"
      },
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "\n",
        "download_file_from_box(train_val_list_url)\n",
        "sleep(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYtV5Q7BiN4A"
      },
      "outputs": [],
      "source": [
        "download_file_from_box(test_list_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages"
      ],
      "metadata": {
        "id": "4B1aclJqzG-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3[extra]"
      ],
      "metadata": {
        "id": "LdMWh2cUzHey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "a0NZ3Ru7wJpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "pjV18hHnxUwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "from torchvision.transforms import transforms"
      ],
      "metadata": {
        "id": "6wrue25lxUMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NIHBreastCancerDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"NIH Breast Cancer Dataset class that can be used by torch Dataloaders.\"\"\"\n",
        "\n",
        "    classes = [\n",
        "        'Atelectasis',\n",
        "        'Cardiomegaly',\n",
        "        'Consolidation',\n",
        "        'Edema',\n",
        "        'Effusion',\n",
        "        'Emphysema',\n",
        "        'Fibrosis',\n",
        "        'Hernia',\n",
        "        'Infiltration',\n",
        "        'Mass',\n",
        "        'No Finding',\n",
        "        'Nodule',\n",
        "        'Pleural_Thickening',\n",
        "        'Pneumonia',\n",
        "        'Pneumothorax'\n",
        "    ]\n",
        "    class_to_id = {class_: id_ for id_, class_ in enumerate(classes)}\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            root_dir: Path,\n",
        "            metadata_path: Path,\n",
        "            data_to_use: Optional[Path] = None,\n",
        "            image_resize_shape: Optional[Tuple[int, int]] = None\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # initialize attributes\n",
        "        self.root_dir = root_dir\n",
        "        self.metadata_path = metadata_path\n",
        "        self.data_to_use = data_to_use\n",
        "        self.image_resize_shape = image_resize_shape or (224, 224)\n",
        "\n",
        "        # initialize the image transformations pipeline\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(self.image_resize_shape),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n",
        "        ])\n",
        "\n",
        "        # parse image paths into a list\n",
        "        self._image_paths = self._parse_root_directory()\n",
        "        # parse features and labels\n",
        "        metadata = self._parse_metadata()\n",
        "        self._img_name_to_features, self._img_name_to_labels = metadata\n",
        "\n",
        "    def _parse_root_directory(self) -> List[Path]:\n",
        "        \"\"\"Parses the contents of the root directory, thus returning all the\n",
        "            image paths.\"\"\"\n",
        "        # list to be returned\n",
        "        src_image_paths = sorted(list(self.root_dir.iterdir()))\n",
        "\n",
        "        # if a list of data to use has been provided\n",
        "        if self.data_to_use is not None:\n",
        "            # read the list\n",
        "            with open(self.data_to_use, 'r') as fp:\n",
        "                valid_image_names = set(fp.read().split('\\n'))\n",
        "\n",
        "            # keep only the images specified in the list\n",
        "            if valid_image_names:\n",
        "                src_image_paths = [\n",
        "                    img_path\n",
        "                    for img_path in src_image_paths\n",
        "                    if img_path.name in valid_image_names\n",
        "                ]\n",
        "\n",
        "        return src_image_paths\n",
        "\n",
        "    def _parse_metadata(self) -> \\\n",
        "            Tuple[Dict[str, Tuple[int, int]], Dict[str, List[int]]]:\n",
        "        \"\"\"Parses the metadata file that contains features and the labels.\"\"\"\n",
        "        # read data and keep useful columns\n",
        "        df = pd.read_csv(self.metadata_path)\n",
        "        df = df[['Image Index', 'Finding Labels',\n",
        "                 'Patient Age', 'Patient Gender']]\n",
        "\n",
        "        # https://www.science.org/doi/10.1126/science.279.5358.1831h\n",
        "        df = df[df['Patient Age'] <= 122]\n",
        "\n",
        "        # convert male-female to binary value (male == 0, female == 1)\n",
        "        df['Patient Gender'] = (df['Patient Gender'] == 'F').astype(int)\n",
        "\n",
        "        # split labels into a list, and then convert them into a list of IDs\n",
        "        df['Finding Labels'] = df['Finding Labels'].str.split('|')\n",
        "        df['Finding Labels'] = df['Finding Labels'].apply(\n",
        "            lambda labels: [self.class_to_id[label] for label in labels]\n",
        "        )\n",
        "\n",
        "        # create a dictionaries for the extra features\n",
        "        img_names_and_features = df[['Image Index', 'Patient Age',\n",
        "                                     'Patient Gender']].values\n",
        "        img_names_to_features = {\n",
        "            img_name: (age, gender)\n",
        "            for img_name, age, gender in img_names_and_features\n",
        "        }\n",
        "\n",
        "        # and for the labels\n",
        "        img_names_and_labels = df[['Image Index', 'Finding Labels']].values\n",
        "        img_name_to_labels = {\n",
        "            img_name: labels for img_name, labels in img_names_and_labels\n",
        "        }\n",
        "\n",
        "        return img_names_to_features, img_name_to_labels\n",
        "\n",
        "    def _read_img(self, item: int) -> torch.Tensor:\n",
        "        img = plt.imread(self._image_paths[item])\n",
        "        if img.ndim == 3:\n",
        "            img = img.mean(axis=2)\n",
        "        img = self.transforms(img)\n",
        "        return (img * 255).to(torch.uint8)\n",
        "\n",
        "    def _get_features(self, item: int) -> torch.Tensor:\n",
        "        img_name = self._image_paths[item].name\n",
        "        return torch.tensor(self._img_name_to_features[img_name],\n",
        "                            dtype=torch.uint8)\n",
        "\n",
        "    def _get_labels(self, item: int) -> List[int]:\n",
        "        img_name = self._image_paths[item].name\n",
        "        return self._img_name_to_labels[img_name]\n",
        "\n",
        "    @staticmethod\n",
        "    def num_classes() -> int:\n",
        "        return len(NIHBreastCancerDataset.classes)\n",
        "\n",
        "    @property\n",
        "    def input_shapes(self) -> Tuple[Tuple[int, int], Tuple[int]]:\n",
        "        return self.image_resize_shape, (self._get_features(0).numel(),)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._image_paths)\n",
        "\n",
        "    def __getitem__(\n",
        "            self,\n",
        "            item: int\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, List[int]]:\n",
        "        return self._read_img(item), self._get_features(item), \\\n",
        "            self._get_labels(item)"
      ],
      "metadata": {
        "id": "pdqzciFTwKYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_directory = Path('all-data')\n",
        "metadata_path = Path('Data_Entry_2017.csv')\n",
        "data_to_use = Path('train_val_list.txt')\n",
        "\n",
        "example_dataset = NIHBreastCancerDataset(\n",
        "    root_dir=root_directory,\n",
        "    metadata_path=metadata_path,\n",
        "    data_to_use=data_to_use\n",
        ")\n",
        "len(example_dataset)"
      ],
      "metadata": {
        "id": "gU99CssswP6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_dataset[35]"
      ],
      "metadata": {
        "id": "nTzlHPFz9E-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment"
      ],
      "metadata": {
        "id": "qDasLVpaxMw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "N0auaFlYxVqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from typing import List, Tuple, Dict, Optional, Callable, Union"
      ],
      "metadata": {
        "id": "RcPk6qC6xNup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A basic reward function for the agent"
      ],
      "metadata": {
        "id": "VaNlq23_xual"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_reward_function(prediction: int, labels: List[int]) -> float:\n",
        "    \"\"\"A basic reward function; Returns +1 if the predicted labels is inside\n",
        "        the list of truth labels, else -1.\"\"\"\n",
        "    return 1 if prediction in labels else -1"
      ],
      "metadata": {
        "id": "jFkU8vqMxw8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NIHBreastCancerEnv(gym.Env):\n",
        "    \"\"\"Class used to represent the NIH Breast Cancer decision Environment.\"\"\"\n",
        "\n",
        "    metadata = {'render_modes': ['human']}\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        reward_function: Callable,\n",
        "        dataset: NIHBreastCancerDataset,\n",
        "        horizon: int = 100,\n",
        "        seed: Optional[int] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        :param reward_function: The function that computes the reward\n",
        "            of an action given the labels.\n",
        "        :param dataset: The dataset from which the env will sample images.\n",
        "        :param horizon: The horizon (max number of steps) of an episode.\n",
        "        :param seed: The random seed that is used for reproducibility. If None,\n",
        "            a random seed will be used.\n",
        "        \"\"\"\n",
        "        super(NIHBreastCancerEnv, self).__init__()\n",
        "        self.seed(seed)\n",
        "\n",
        "        # initialize important variables\n",
        "        self.reward_function = reward_function\n",
        "        self.dataset = dataset\n",
        "        self.horizon = horizon\n",
        "        self.default_seed = seed\n",
        "\n",
        "        # initialize state and action spaces\n",
        "        img_shape, features_shape = dataset.input_shapes\n",
        "        num_classes = dataset.num_classes()\n",
        "        self.observation_space = gym.spaces.Dict({\n",
        "            'image': gym.spaces.Box(low=0,\n",
        "                                    high=255,\n",
        "                                    shape=(3, *img_shape),\n",
        "                                    dtype=np.uint8),\n",
        "            'features': gym.spaces.Box(low=np.array([0, 0]),\n",
        "                                       high=np.array([122, 1]),\n",
        "                                       shape=features_shape,\n",
        "                                       dtype=np.uint8)\n",
        "        })\n",
        "        self.action_space = gym.spaces.Discrete(num_classes)\n",
        "        self.action_space.seed(seed)\n",
        "\n",
        "        # define the images that will appear in this episode\n",
        "        self.states: List[int] = []\n",
        "\n",
        "        # define the maximum episode length\n",
        "        self.current_episode: int = 0\n",
        "\n",
        "    def _initialize_episode_data(self, seed: Optional[int] = None) -> List[int]:\n",
        "        \"\"\"Creates a list with the indices of the samples to be used in\n",
        "            the current episode.\"\"\"\n",
        "        generator = np.random.RandomState(seed=seed)\n",
        "        indices = generator.choice(len(self.dataset), self.horizon, replace=False)\n",
        "        return indices.tolist()\n",
        "\n",
        "    @property\n",
        "    def terminal_state(self) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Returns a dummy terminal state, where all features are 0.\"\"\"\n",
        "        img_shape, features_shape = self.dataset.input_shapes\n",
        "        return {'image': np.zeros(img_shape),\n",
        "                'features': np.zeros(features_shape)}\n",
        "\n",
        "    def _sample(self, item: int) -> \\\n",
        "            Tuple[np.ndarray, np.ndarray, List[int]]:\n",
        "        \"\"\"Returns the processed data of a state.\"\"\"\n",
        "        assert item < len(self.states)\n",
        "        image, features, labels = self.dataset[self.states[item]]\n",
        "        return image.detach().cpu().numpy(), \\\n",
        "            features.detach().cpu().numpy(), \\\n",
        "            labels\n",
        "\n",
        "    def observe(self) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Returns an observation of the current state.\"\"\"\n",
        "        image, features, _ = self._sample(0)\n",
        "        return {'image': image, 'features': features}\n",
        "\n",
        "    def done(self) -> bool:\n",
        "        \"\"\"Returns True if the environment is in a terminal state;\n",
        "            Else False.\"\"\"\n",
        "        return len(self.states) == 0 or self.current_episode == self.horizon\n",
        "\n",
        "    def reset(\n",
        "            self,\n",
        "            *,\n",
        "            seed: Optional[int] = None,\n",
        "            return_info: bool = False,\n",
        "            options: Optional[dict] = None,\n",
        "    ) -> Union[Dict[str, np.ndarray], Tuple[Dict[str, np.ndarray], Dict]]:\n",
        "        \"\"\"Resets the environment to its initial state and returns it.\"\"\"\n",
        "        self.states = self._initialize_episode_data(seed)\n",
        "        self.current_episode = 0\n",
        "        obs = self.observe()\n",
        "        return (obs, {}) if return_info else obs\n",
        "\n",
        "    def step(\n",
        "            self,\n",
        "            action: int\n",
        "    ) -> Tuple[Dict[str, np.ndarray], float, bool, Dict]:\n",
        "        \"\"\"Performs one step in the environment, by applying the\n",
        "            specified action.\"\"\"\n",
        "        # get the data of the current state and then remove it\n",
        "        _, _, labels = self._sample(0)\n",
        "        del self.states[0]\n",
        "\n",
        "        # advance one time step\n",
        "        self.current_episode += 1\n",
        "\n",
        "        # the next state is the (image, features) in the start of the list\n",
        "        next_state = self.observe() if not self.done() else self.terminal_state\n",
        "        # get the reward from the reward function\n",
        "        reward = self.reward_function(action, labels)\n",
        "        # whether the environment has reached a terminal state\n",
        "        done = True if self.done() else False\n",
        "        # the info dictionary\n",
        "        info = {}\n",
        "\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        \"\"\"Renders the environment. Used for visualization,\n",
        "            currently not used.\"\"\"\n",
        "        pass"
      ],
      "metadata": {
        "id": "guYO_8TpxboV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_env() -> NIHBreastCancerEnv:\n",
        "    return NIHBreastCancerEnv(\n",
        "        reward_function=basic_reward_function,\n",
        "        dataset=example_dataset,\n",
        "        horizon=1000\n",
        "    )\n",
        "\n",
        "num_vec_envs = 4\n",
        "env_initializers = [create_env for _ in range(num_vec_envs)]\n",
        "vec_envs = DummyVecEnv(env_initializers)\n",
        "\n",
        "assert len(vec_envs.envs) == num_vec_envs\n",
        "vec_envs.reset()\n",
        "\n",
        "result = vec_envs.step(np.arange(num_vec_envs))\n",
        "print(result[0]['image'].shape, result[0]['features'].shape)  # next states\n",
        "print(result[1].shape)  # rewards\n",
        "print(result[2].shape)  # dones\n",
        "print(len(result[3]))  # infos"
      ],
      "metadata": {
        "id": "2UeBb-R0xnhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extractor"
      ],
      "metadata": {
        "id": "rzLSOD66yimd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "qFGYSxVgy6on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.fx as fx\n",
        "\n",
        "from typing import Dict, Tuple, Union, Optional\n",
        "from torchvision.models import feature_extraction\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor"
      ],
      "metadata": {
        "id": "lXA2GUyKy705"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrapper model for a torch feature extractor"
      ],
      "metadata": {
        "id": "9NYr2r2tzNdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureSubnetWrapper(nn.Module):\n",
        "    \"\"\"Wraps a feature extractor so that it can be used directly without\n",
        "        instantiating the desired layer.\"\"\"\n",
        "\n",
        "    def __init__(self, model: fx, until_layer: str) -> None:\n",
        "        super(FeatureSubnetWrapper, self).__init__()\n",
        "        self.model = model\n",
        "        self.until_layer = until_layer\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)[self.until_layer]"
      ],
      "metadata": {
        "id": "guuPRimtzQh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Base CNN model used to extract features from images"
      ],
      "metadata": {
        "id": "wbqwUO-sy2ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCombinedExtractor(BaseFeaturesExtractor):\n",
        "    \"\"\"Custom Feature Extraction Module.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            observation_space: gym.spaces.Dict,\n",
        "            device: torch.device,\n",
        "            model_name: str,\n",
        "            weights: Optional[str],\n",
        "            until_layer: Union[int, str],\n",
        "            freeze_pretrained_model: bool,\n",
        "            img_dims: Tuple[int, int],\n",
        "            num_dataset_features: int,\n",
        "            n_classes: int\n",
        "    ) -> None:\n",
        "        super(CustomCombinedExtractor, self).__init__(observation_space,\n",
        "                                                      features_dim=1)\n",
        "\n",
        "        # define the device where the models will be placed on\n",
        "        self.device = device\n",
        "\n",
        "        # define some important variables\n",
        "        self.img_dims = img_dims\n",
        "        self.num_dataset_features = num_dataset_features\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # define the feature extractors\n",
        "        cnn_extractor, mlp_extractor = self._get_feature_extractors(\n",
        "            model_name=model_name,\n",
        "            weights=weights,\n",
        "            freeze_pretrained_model=freeze_pretrained_model,\n",
        "            until_layer=until_layer\n",
        "        )\n",
        "        extractors = {'image': cnn_extractor, 'features': mlp_extractor}\n",
        "        self.extractors = nn.ModuleDict(extractors)\n",
        "\n",
        "        # do a forward pass in the CNN to see output shape\n",
        "        cnn_out_shape = self.get_model_out_size(\n",
        "            model=cnn_extractor,\n",
        "            img_dims=img_dims,\n",
        "            device=device\n",
        "        )\n",
        "        # get the MLP output size in order to compute the final features dim\n",
        "        mlp_out_size = mlp_extractor[-1].weight.shape[0]\n",
        "\n",
        "        # features dimension\n",
        "        self._features_dim = cnn_out_shape + mlp_out_size\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            observations: Union[Dict[str, torch.Tensor], torch.Tensor]\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Extract features and concatenate them in a Tensor.\"\"\"\n",
        "        # extract features and place them in a list\n",
        "        extractors_out = [extractor(observations[key])\n",
        "                          for key, extractor in self.extractors.items()]\n",
        "\n",
        "        # concatenate extracted features to get out shape:\n",
        "        #   (B, self._features_dim)\n",
        "        latent_vector = torch.cat(extractors_out, dim=1)\n",
        "        return latent_vector\n",
        "\n",
        "    def load_pretrained_model(self, model_name: str, weights: str) -> nn.Module:\n",
        "        \"\"\"Loads or Downloads a pretrained CNN model and returns it.\"\"\"\n",
        "        \n",
        "        # some issues arise with hubconf.py, catch the error, fix it and retry\n",
        "        try:\n",
        "            model = torch.hub.load('pytorch/vision', model_name, weights=weights)\n",
        "        except:\n",
        "            os.system(\"\"\"sed -i 's/from torchvision.models import get_model_weights, get_weight/from torchvision.models import get_weight/g' ~/.cache/torch/hub/pytorch_vision_main/hubconf.py\"\"\")\n",
        "            model = torch.hub.load('pytorch/vision', model_name, weights=weights)\n",
        "        return model.to(self.device)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_feature_subnet(\n",
        "            model: nn.Module,\n",
        "            until_layer: Union[int, str]\n",
        "    ) -> nn.Module:\n",
        "        \"\"\"Given a pretrained model, it returns the model that constructs the\n",
        "            features, having removed layers from the end.\"\"\"\n",
        "        # check for correctness and ensure \"until_layer\" is a string\n",
        "        model_layers = feature_extraction.get_graph_node_names(model)[0]\n",
        "        if isinstance(until_layer, str):\n",
        "            if until_layer not in model_layers:\n",
        "                raise RuntimeError(f'Layer \"{until_layer}\" not in model.')\n",
        "        elif isinstance(until_layer, int):\n",
        "            until_layer = model_layers[:-until_layer]\n",
        "        else:\n",
        "            raise RuntimeError(f'Wrong type for \"until_layer\" argument.')\n",
        "\n",
        "        # get the feature extractor and return it\n",
        "        feature_extractor = feature_extraction.create_feature_extractor(\n",
        "            model,\n",
        "            return_nodes=[until_layer]\n",
        "        )\n",
        "        return FeatureSubnetWrapper(feature_extractor, until_layer)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_model_out_size(\n",
        "            model: nn.Module,\n",
        "            img_dims: Tuple[int, int],\n",
        "            device: torch.device\n",
        "    ) -> int:\n",
        "        \"\"\"Returns the output size that is produced by a model.\"\"\"\n",
        "        dims = (1, 3, img_dims[0], img_dims[1])\n",
        "        return model(torch.zeros(dims).to(device)).shape[1]\n",
        "\n",
        "    @staticmethod\n",
        "    def get_layer_out_size(\n",
        "            model: nn.Module,\n",
        "            img_dims: Tuple[int, int],\n",
        "            until_layer: Union[int, str],\n",
        "            device: torch.device\n",
        "    ) -> int:\n",
        "        \"\"\"Returns the output size that is produced by a layer of a model.\"\"\"\n",
        "        feature_subnet = CustomCombinedExtractor.get_feature_subnet(\n",
        "            model,\n",
        "            until_layer\n",
        "        )\n",
        "        return CustomCombinedExtractor.get_model_out_size(\n",
        "            feature_subnet,\n",
        "            img_dims,\n",
        "            device\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def freeze_model(model: nn.Module) -> None:\n",
        "        \"\"\"Freezes a PyTorch model, i.e. makes it non trainable.\"\"\"\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        model.eval()\n",
        "\n",
        "    def _get_feature_extractors(\n",
        "            self,\n",
        "            model_name: str,\n",
        "            weights: Optional[str],\n",
        "            freeze_pretrained_model: bool = True,\n",
        "            until_layer: Union[int, str] = None\n",
        "    ) -> Tuple[nn.Module, nn.Module]:\n",
        "        \"\"\"Returns the NNs used to extract features from observations.\"\"\"\n",
        "        # load the model and get the output until the specified layer\n",
        "        model = self.load_pretrained_model(model_name, weights=weights)\n",
        "        feature_subnet = self.get_feature_subnet(\n",
        "            model,\n",
        "            until_layer=until_layer\n",
        "        )\n",
        "\n",
        "        # flatten the outputs (might not be needed)\n",
        "        feature_subnet = nn.Sequential(\n",
        "            feature_subnet,\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # if specified, freeze the weights of the pre-trained model\n",
        "        if freeze_pretrained_model:\n",
        "            self.freeze_model(feature_subnet)\n",
        "\n",
        "        # create the MLP for the other dataset features\n",
        "        feat_mlp = nn.Sequential(\n",
        "            nn.Linear(in_features=self.num_dataset_features, out_features=64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=64, out_features=32),\n",
        "        )\n",
        "\n",
        "        return feature_subnet.to(self.device), feat_mlp.to(self.device)"
      ],
      "metadata": {
        "id": "1BsOk8sey1sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN"
      ],
      "metadata": {
        "id": "uUiJZCdj1_3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN Agent"
      ],
      "metadata": {
        "id": "vc-4HpAK2dNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "3IUsEBhj2fEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "from stable_baselines3 import DQN\n",
        "from typing import Tuple, Dict, Union, Optional\n",
        "from stable_baselines3.dqn.policies import DQNPolicy\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv"
      ],
      "metadata": {
        "id": "tOoIES032epH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    \"\"\"Wrapper class for the DQN agent of Stable Baselines 3.\"\"\"\n",
        "\n",
        "    net_arch = [500, 500, 250, 50]\n",
        "    activation_fn = torch.nn.ReLU\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            envs: DummyVecEnv = None,\n",
        "            device: torch.device = None,\n",
        "            cnn: str = None,\n",
        "            weights: str = None,\n",
        "            load_pretrained_agent: bool = False,\n",
        "            pretrained_agent_path: Path = None,\n",
        "            freeze_pretrained_model: bool = True,\n",
        "            img_dims: Tuple[int, int] = (224, 224),\n",
        "            num_dataset_features: int = 2,\n",
        "            n_classes: int = 15,\n",
        "            until_layer: Union[str, int] = None,\n",
        "            learning_rate: float = 1e-4,\n",
        "            buffer_size: int = 1e3,\n",
        "            learning_starts: int = 1e3,\n",
        "            batch_size: int = 32,\n",
        "            tau: float = 1.0,\n",
        "            gamma: float = 0.99,\n",
        "            train_freq: int = 4,\n",
        "            gradient_steps: int = 1,\n",
        "            target_update_interval: int = 1e4,\n",
        "            max_grad_norm: float = 10,\n",
        "            tensorboard_log: Optional[Path] = None,\n",
        "            verbose: int = 1\n",
        "    ) -> None:\n",
        "        # set variables\n",
        "        self.envs = envs\n",
        "\n",
        "        # either load a pretrained agent or create a new one from scratch\n",
        "        if load_pretrained_agent:\n",
        "            self.model = DQN.load(pretrained_agent_path, device=device)\n",
        "            print(f'Loaded model from path: {pretrained_agent_path}.')\n",
        "        else:\n",
        "            # specify model\n",
        "            feature_extractor_kwargs = dict(\n",
        "                device=device,\n",
        "                model_name=cnn,\n",
        "                weights=weights,\n",
        "                freeze_pretrained_model=freeze_pretrained_model,\n",
        "                img_dims=img_dims,\n",
        "                num_dataset_features=num_dataset_features,\n",
        "                n_classes=n_classes,\n",
        "                until_layer=until_layer\n",
        "            )\n",
        "            policy_kwargs = dict(\n",
        "                features_extractor_class=CustomCombinedExtractor,\n",
        "                features_extractor_kwargs=feature_extractor_kwargs,\n",
        "                net_arch=self.net_arch,\n",
        "                activation_fn=self.activation_fn,\n",
        "                normalize_images=True\n",
        "            )\n",
        "            self.model = DQN(\n",
        "                DQNPolicy,\n",
        "                self.envs,\n",
        "                learning_rate=learning_rate,\n",
        "                buffer_size=buffer_size,\n",
        "                learning_starts=learning_starts,\n",
        "                batch_size=batch_size,\n",
        "                tau=tau,\n",
        "                gamma=gamma,\n",
        "                train_freq=train_freq,\n",
        "                gradient_steps=gradient_steps,\n",
        "                target_update_interval=target_update_interval,\n",
        "                max_grad_norm=max_grad_norm,\n",
        "                policy_kwargs=policy_kwargs,\n",
        "                tensorboard_log=tensorboard_log,\n",
        "                verbose=verbose,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "    def learn(self, timesteps: int) -> None:\n",
        "        \"\"\"Learn for a specific number of timesteps, unless\n",
        "            the early stopping callback kicks in.\"\"\"\n",
        "        start = time.time()\n",
        "        self.model.learn(total_timesteps=timesteps)\n",
        "        print(f'\\nTime for training: {time.time() - start:.2f} seconds.\\n')\n",
        "\n",
        "    def choose_action(self, observation: Dict[str, np.ndarray]) -> int:\n",
        "        \"\"\"Chooses (and returns) an action, given a specific observation.\"\"\"\n",
        "        return int(self.model.predict(observation)[0])\n",
        "\n",
        "    def get_scores(self, observation: Dict[str, np.ndarray]) -> np.ndarray:\n",
        "        \"\"\"Returns the predicted Q-values for the given state.\"\"\"\n",
        "        observation, _ = self.model.policy.obs_to_tensor(observation)\n",
        "        with torch.no_grad():\n",
        "            return self.model.policy.q_net(observation).cpu().numpy()\n",
        "\n",
        "    def save(self, save_path: Path) -> None:\n",
        "        self.model.save(save_path)\n",
        "        print(f'Saved the model at path: {save_path}.')"
      ],
      "metadata": {
        "id": "RTgFpLYC2hvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN Main"
      ],
      "metadata": {
        "id": "qqiPzN6n3nT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fixed variables"
      ],
      "metadata": {
        "id": "v9eoQDHv3zYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_directory = Path('all-data')\n",
        "metadata_path = Path('Data_Entry_2017.csv')\n",
        "data_to_use = Path('train_val_list.txt')\n",
        "image_resize_shape = (384, 384)"
      ],
      "metadata": {
        "id": "uSxkhcu93pMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment Hyperparameters"
      ],
      "metadata": {
        "id": "j2-Hs3DN4JEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 1000\n",
        "num_vec_envs = 2"
      ],
      "metadata": {
        "id": "p74_5saE4GpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent Hyperparameters"
      ],
      "metadata": {
        "id": "oViWzEKY4Nxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#stable_baselines3.dqn.DQN\n",
        "\n",
        "cnn_extractor_name = 'vit_b_16'\n",
        "cnn_extractor_weights_name = 'IMAGENET1K_SWAG_E2E_V1'\n",
        "until_layer = 'getitem_5'\n",
        "total_timesteps = 50  # ToDo: CHANGE THIS TO TRAIN FOR LONGER :)\n",
        "freeze_pretrained_cnn = True\n",
        "learning_rate = 1e-4\n",
        "buffer_size = 1_000\n",
        "learning_starts = 10\n",
        "batch_size = 32\n",
        "tau = 1.0\n",
        "gamma = 0.99\n",
        "train_freq = 4\n",
        "gradient_steps = 1\n",
        "target_update_interval = 2\n",
        "max_grad_norm = 10"
      ],
      "metadata": {
        "id": "QdK5eDhh4P9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorboard Logs directory"
      ],
      "metadata": {
        "id": "BwrMhOhp4R9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard_logdir = Path('DQN-logs')"
      ],
      "metadata": {
        "id": "1MxNWl3f4TeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Glue everything together"
      ],
      "metadata": {
        "id": "mSOsotg54a_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nih_dataset = NIHBreastCancerDataset(\n",
        "    root_dir=root_directory,\n",
        "    metadata_path=metadata_path,\n",
        "    data_to_use=data_to_use,\n",
        "    image_resize_shape=image_resize_shape\n",
        ")\n",
        "\n",
        "def create_env() -> NIHBreastCancerEnv:\n",
        "    return NIHBreastCancerEnv(\n",
        "        reward_function=basic_reward_function,\n",
        "        dataset=nih_dataset,\n",
        "        horizon=horizon\n",
        "    )\n",
        "\n",
        "env_initializers = [create_env for _ in range(num_vec_envs)]\n",
        "vec_envs = DummyVecEnv(env_initializers)"
      ],
      "metadata": {
        "id": "Bev7j6Gh4d7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the agent and start training"
      ],
      "metadata": {
        "id": "zNcR0PmH4yE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DQNAgent(\n",
        "    envs=vec_envs,\n",
        "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    cnn=cnn_extractor_name,\n",
        "    weights=cnn_extractor_weights_name,\n",
        "    freeze_pretrained_model=freeze_pretrained_cnn,\n",
        "    img_dims=nih_dataset.input_shapes[0],\n",
        "    n_classes=nih_dataset.num_classes(),\n",
        "    until_layer=until_layer,\n",
        "    learning_rate=learning_rate,\n",
        "    buffer_size=buffer_size,\n",
        "    learning_starts=learning_starts,\n",
        "    batch_size=batch_size,\n",
        "    tau=tau,\n",
        "    gamma=gamma,\n",
        "    train_freq=train_freq,\n",
        "    gradient_steps=gradient_steps,\n",
        "    target_update_interval=target_update_interval,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    tensorboard_log=tensorboard_logdir,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "gPKrSakd40IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.learn(timesteps=total_timesteps)"
      ],
      "metadata": {
        "id": "Jf_hxbWh43Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "R8AqKQKo5ZLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some utilities for evaluation"
      ],
      "metadata": {
        "id": "vi9AL43m5fmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from typing import Tuple, List\n",
        "\n",
        "\n",
        "def get_predictions_on_test_env(\n",
        "        agent: DQNAgent,\n",
        "        test_env: NIHBreastCancerEnv\n",
        ") -> Tuple[List[np.ndarray], List[List[int]]]:\n",
        "    \"\"\"Runs the agent on the test environment and returns the predictions\n",
        "        along with the truth labels.\"\"\"\n",
        "    # lists to be returned\n",
        "    predictions, truth_labels = [], []\n",
        "\n",
        "    # run simulation\n",
        "    done = False\n",
        "    obs = test_env.reset()\n",
        "    while not done:\n",
        "        _, _, labels = test_env._sample(0)\n",
        "        values = agent.get_scores(obs)\n",
        "        predictions.append(values)\n",
        "        truth_labels.append(labels)\n",
        "        obs, _, done, _ = test_env.step(0)\n",
        "\n",
        "    return predictions, truth_labels\n",
        "\n",
        "\n",
        "def adapt_truth_labels(\n",
        "        all_logits: List[np.ndarray],\n",
        "        truth_labels: List[List[int]],\n",
        "        num_classes: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Returns a new list of labels such that only one label exists for\n",
        "        each data point. The label of a data point corresponds to the\n",
        "        actual label if predicted correctly by the model, else a random\n",
        "        label different from the one the model predicted.\"\"\"\n",
        "    new_labels = []\n",
        "    for logits, labels in zip(all_logits, truth_labels):\n",
        "        pred_label = logits.argmax()\n",
        "        if pred_label in labels:  # predicted correctly -> argmax == truth\n",
        "            new_labels.append(pred_label)\n",
        "        else:  # predicted wrong -> argmax != truth\n",
        "            wrong_labels = set(range(num_classes)).difference(labels)\n",
        "            wrong_label = np.random.choice(list(wrong_labels))\n",
        "            new_labels.append(wrong_label)\n",
        "    return np.array(new_labels)"
      ],
      "metadata": {
        "id": "YcHfNYJD5aer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create test dataset and test environment"
      ],
      "metadata": {
        "id": "e_7OQyAK5twa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_root_directory = Path('all-data')\n",
        "test_data_to_use = Path('test_list.txt')\n",
        "\n",
        "nih_test_dataset = NIHBreastCancerDataset(\n",
        "    root_dir=test_root_directory,\n",
        "    metadata_path=metadata_path,\n",
        "    data_to_use=test_data_to_use,\n",
        "    image_resize_shape=image_resize_shape\n",
        ")\n",
        "\n",
        "test_env = NIHBreastCancerEnv(\n",
        "    reward_function=basic_reward_function,\n",
        "    dataset=nih_test_dataset,\n",
        "    # horizon=len(test_dataset)  # Uncomment this line when evaluating\n",
        "    horizon = 10  # this is for demonstration purposes, remove it when evaluating\n",
        ")"
      ],
      "metadata": {
        "id": "HLwDGTjR5p6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate"
      ],
      "metadata": {
        "id": "vG1C-SCJ6flH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the predictions of the model on the test set\n",
        "all_logits, all_truth_labels = get_predictions_on_test_env(agent, test_env)\n",
        "\n",
        "# get the actual actions with argmax\n",
        "pred_labels = np.array([logits.argmax() for logits in all_logits])\n",
        "\n",
        "# adapt the truth labels to match the multiclass setting for AUC\n",
        "num_classes = len(NIHBreastCancerDataset.classes)\n",
        "adapted_labels = adapt_truth_labels(\n",
        "    all_logits,\n",
        "    all_truth_labels,\n",
        "    num_classes=num_classes\n",
        ")"
      ],
      "metadata": {
        "id": "tgKZDy_P6b6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute various metrics"
      ],
      "metadata": {
        "id": "AS6_vyrz6lTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(adapted_labels, pred_labels)\n",
        "f1 = f1_score(adapted_labels, pred_labels, average='micro')"
      ],
      "metadata": {
        "id": "lMIvqO856dxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print statistics"
      ],
      "metadata": {
        "id": "7loSDC-56t1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('----------    DQN Performance    ----------')\n",
        "print(f'\\tAccuracy: {accuracy:.2f}')\n",
        "print(f'\\tF1 Score: {f1:.2f}')"
      ],
      "metadata": {
        "id": "a3xg8FJg6rum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't forget to check the tensorboard logs for the plots!"
      ],
      "metadata": {
        "id": "ZoxCLCuOD2k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir DQN-logs"
      ],
      "metadata": {
        "id": "SgI7nX8DPUyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model"
      ],
      "metadata": {
        "id": "qwdv9_SD6xZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = Path('DQN-model')\n",
        "agent.save(save_path)"
      ],
      "metadata": {
        "id": "Du6VSnDp6xxF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}